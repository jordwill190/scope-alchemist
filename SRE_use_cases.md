### SRE Service Resiliency Use Cases by Application Priority

Site Reliability Engineering (SRE) focuses on building and maintaining reliable systems through practices like defining Service Level Objectives (SLOs), error budgets, monitoring, and fault-tolerant architectures. Resiliency use cases emphasize patterns such as redundancy, fault isolation, recovery mechanisms, and proactive testing to minimize downtime and ensure business continuity. These are tailored to application priority levels based on impact: 

- **Critical Business Applications**: Mission-critical systems (e.g., core banking or e-commerce checkout) with zero-tolerance for downtime, requiring 99.99%+ availability and near-zero Recovery Time Objectives (RTO).
- **Pri-1 Applications**: High-impact services (e.g., customer authentication) with severe business disruption if failed, targeting 99.9% availability and RTO under 15 minutes.
- **Pri-2 Applications**: Significant but non-catastrophic impact (e.g., reporting dashboards), aiming for 99.5% availability and RTO under 1 hour.
- **Pri-3 Applications**: Lower-impact tools (e.g., internal admin portals), with 99% availability and RTO up to 4 hours acceptable.

The following table outlines key SRE resiliency use cases, drawn from established patterns like those in AWS, Azure, and Google SRE practices. Each includes a brief description, implementation example, and rationale for the priority level.

| Priority Level       | Use Case                          | Description                                                                 | Example Implementation                                                                 | Rationale/Benefits |
|----------------------|-----------------------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------------------|---------------------|
| **Critical Business** | Multi-Region Active-Active Deployment | Run workloads simultaneously across multiple regions for zero-downtime failover and data replication. | Deploy core banking app across AWS us-east-1 and us-west-2 with active-active replication via Amazon RDS Multi-AZ and Global Accelerator for traffic routing. | Ensures near-zero RTO/RPO; withstands full regional outages, protecting revenue-critical operations with high complexity trade-off justified by zero-tolerance needs. |
| **Critical Business** | Chaos Engineering with Active-Active Redundancy | Intentionally inject failures to test system resilience in production-like environments. | Use tools like Chaos Monkey to simulate node failures in a payment processing microservices architecture, verifying active-active redundancy across replicas. | Builds confidence in high-availability (99.99%+) setups; identifies weaknesses proactively for mission-critical systems. |
| **Critical Business** | Circuit Breaker with Geode Pattern | Halt calls to failing dependencies and enable automatic recovery to prevent cascading failures. | Implement Hystrix-like circuit breakers in a financial transaction service, paired with cross-region data replication for failover. | Protects against overload in zero-downtime scenarios; supports self-healing for always-on services. |
| **Critical Business** | Event Sourcing for State Recovery | Store state as immutable event logs for full reconstruction post-failure. | Log all user actions in an e-commerce order system using Apache Kafka, allowing replay for instant recovery during outages. | Guarantees data integrity and rapid recovery; essential for audit-heavy, high-stakes environments. |
| **Pri-1**            | Multi-AZ with Multi-Region DR (Pilot Light/Warm Standby) | Pre-provision infrastructure in secondary regions for quick scaling during primary failures. | Set up a customer auth service with Pilot Light DR: replicate data to a warm standby region, scaling to full capacity in minutes via Auto Scaling. | Achieves RTO in minutes; balances cost with high reliability for severe-impact services. |
| **Pri-1**            | Bulkhead Isolation with Retry Logic | Segment resources to contain faults and retry transient errors exponentially. | Use thread pools in a Java app for API calls, isolating database connections and implementing jittered retries for network blips. | Limits blast radius; improves fault tolerance for authentication flows without full outages. |
| **Pri-1**            | Priority Queue for Task Orchestration | Process high-priority tasks first during overloads using queue-based leveling. | Integrate Amazon SQS with priority levels in a notification service, ensuring critical alerts queue ahead of non-urgent ones. | Maintains key functions during spikes; aligns with SLOs for high-impact workloads. |
| **Pri-1**            | Health Endpoint Monitoring with Leader Election | Monitor component health and elect failover leaders dynamically. | Expose /healthz endpoints in Kubernetes pods for a search service, using etcd for leader election on failure. | Enables proactive self-healing; critical for maintaining 99.9% uptime in distributed systems. |
| **Pri-2**            | Multi-AZ with Static Stability | Distribute instances across AZs with load balancing for seamless failover. | Deploy a reporting dashboard on EC2 Auto Scaling groups across two AZs, using ELB to redirect traffic from failed instances. | Handles AZ failures without downtime; cost-effective for moderate-impact apps. |
| **Pri-2**            | Cache-Aside with Queue-Based Load Leveling | Cache frequent reads and buffer requests to smooth traffic spikes. | Use Redis for caching user profiles in a dashboard app, queuing excess requests via RabbitMQ for batched processing. | Reduces origin load; ensures availability during peaks for non-catastrophic services. |
| **Pri-2**            | Gateway Routing with Rate Limiting | Route to healthy backends and throttle requests to prevent overload. | Implement API Gateway to route analytics queries and limit rates to 1000/min per user. | Controls traffic; prevents degradation in significant but recoverable scenarios. |
| **Pri-2**            | Competing Consumers for Distributed Processing | Use multiple replicas to process queues redundantly. | Deploy consumer pods in Kubernetes to handle log aggregation queues, failing over on instance loss. | Builds redundancy; supports scalability for medium-priority workloads. |
| **Pri-3**            | Basic Multi-AZ Redundancy | Deploy across AZs with auto-recovery for simple failures. | Run an internal wiki on single EC2 in an Auto Scaling group spanning one AZ pair for basic failover. | Low-cost fault tolerance; sufficient for minor disruptions. |
| **Pri-3**            | Ambassador Pattern for Network Resilience | Offload retries and buffering to a proxy layer. | Use Envoy proxy in front of a low-priority API to handle transient network issues automatically. | Simplifies client code; adds basic resiliency without high overhead. |
| **Pri-3**            | Sharding with Health Checks | Partition data across nodes and monitor for rerouting. | Shard user data in a non-critical CRM tool across database nodes, using health checks to redirect queries. | Isolates failures; economical for low-impact apps. |
| **Pri-3**            | Strangler Fig for Incremental Modernization | Gradually replace legacy components to avoid big-bang risks. | Migrate an old admin portal by routing subsets of traffic to new microservices over time. | Minimizes migration downtime; suitable for tolerable interruptions. |
